Music plays a crucial role in human emotions, influencing mood, behaviour, and even physiological responses. This project aims to develop a music emotion classification system capable of recognizing and categorizing emotions in music while also modifying emotional tone through to- nality-based transposition. The goal is to enhance human-music interaction by classifying the emotional im- pact of songs and altering their tonality to achieve a different emotional expression. Using the EMOPIA and DEAM datasets, we will train machine learning models from scratch, progressing from basic classification techniques like K-Nearest Neighbors (KNN) to more ad- vanced deep learning models such as LSTM (Long Short-Term Memory). Additionally, we might use the Librosa library for feature extraction and music pro-cessing, ensuring accurate identification of key musical characteristics that contribute to emotion perception. The final product will be an interactive web application that allows users to analyze and manipulate music emotions dynamically.
